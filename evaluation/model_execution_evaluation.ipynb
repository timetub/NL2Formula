{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlwings as xw\n",
    "from xlwings import view\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,os\n",
    "import read\n",
    "\n",
    "from pprint import pprint\n",
    "res_file=''##res_file_path\n",
    "\n",
    "\n",
    "with open('test.json') as file:\n",
    "    data = json.load(file)\n",
    "with open(res_file) as file:\n",
    "    data_pre = json.load(file)\n",
    "fail_list_all=[]\n",
    "success_list_all=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def read_json_files_in_folder(folder_path):\n",
    "    json_data = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, \"r\") as json_file:\n",
    "                try:\n",
    "                    data = json.load(json_file)\n",
    "                    json_data.append(data)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON in file '{file_path}': {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file '{file_path}': {e}\")\n",
    "    return json_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"lunwen_result\"\n",
    "    json_data_list   = read_json_files_in_folder(folder_path)\n",
    "    print(\"Number of JSON files read:\", len(json_data_list))\n",
    "    # Do further processing with the json_data_list if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_data = json.dumps(json_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test=[]\n",
    "for item in json_data_list:\n",
    "    data_test+=item\n",
    "data_pre=data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_pref=[data_pre[i] for i in range(len(data_pre))\n",
    "                            if data_pre[i]['level']!='Calculation']\n",
    "data_prec=[data_pre[i] for i in range(len(data_pre))\n",
    "                            if data_pre[i]['level']=='Calculation']\n",
    "data_p=data_pref+data_prec\n",
    "result=[]\n",
    "for j in range(len(data)):\n",
    "    for i in range(len(data[j]['t5Formulas'])):\n",
    "        data[j]['t5Formulas'][i]['Table']=data[j]['Table']\n",
    "        result.append(data[j]['t5Formulas'][i])\n",
    "        \n",
    "len(result),len(data_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_p_dict = {item['question'].split(\" |\")[0]: index for index, item in enumerate(data_p)}\n",
    "x=0\n",
    "test_list=[]\n",
    "for i in range(len(result)):\n",
    "    question = result[i]['Question']\n",
    "    if question in data_p_dict:\n",
    "        data_index = data_p_dict[question]\n",
    "        result[i]['pre_f'] = data_p[data_index]['prediction']\n",
    "        test_list.append(result[i])\n",
    "        x+=1\n",
    "        # print(x)\n",
    "    else:\n",
    "        print(f\"No matching context found for question: {question}\")\n",
    "len(test_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gpt_eval_3000.json','w') as file:\n",
    "    json.dump(random_elements,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gpt_eval_3000.json','r') as file:\n",
    "    data=json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = read.read_json(os.path.join('configuration.json'))['OpenAI_api_key']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read.read_json(os.path.join('configuration.json'))['OpenAI_api_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_formular(question: str,table, model='gpt-3.5-turbo'):\n",
    "    llm = ChatOpenAI(model_name=model, max_tokens=50,)\n",
    "    prompt = \"Write a excel formula to answer the quesion based on table beleow\\n\"\\\n",
    "        \"You can use functions like SUM,XLOOKUP,UNIQUE,AVERAGE,etc...\\n\"\\\n",
    "             \"Question:{question}\\n\" \\\n",
    "             \"Here is the Table: {table}.\\n\" \\\n",
    "             \"{format_instructions}\"\n",
    "             \n",
    "    prompt_template = ChatPromptTemplate.from_template(prompt)\n",
    "    \n",
    "    #options = ResponseSchema(name=\"words\", description=\"The words are related to bad behavior.\")\n",
    "    options = ResponseSchema(name=\"Formula\", description=\"The formula is related to question\")\n",
    "    response_schemas = [options]\n",
    "    output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "    form_instructions = output_parser.get_format_instructions()\n",
    "    \n",
    "    final_prompt = prompt_template.format_messages(question=question, table=table, format_instructions=form_instructions)\n",
    "    print(final_prompt)\n",
    "    output = llm(final_prompt)\n",
    "    return output_parser.parse(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iterations = 5000  # Set a maximum number of iterations to avoid infinite loop\n",
    "\n",
    "for iteration in range(max_iterations):\n",
    "    try:\n",
    "        for i, item in enumerate(data[:]):\n",
    "            # if 'llmFormula' not in item:\n",
    "            if 1:\n",
    "                print(i)\n",
    "                llmout = get_formular(item[\"Question\"], item[\"Table\"])\n",
    "                item['llmFormula'] = llmout['Formula']\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    else:\n",
    "        # The loop completed without errors, break out of the while loop\n",
    "        break\n",
    "else:\n",
    "    print(\"Maximum number of iterations reached, exiting loop.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_execution(sheet,item,table,f1:str,f2:str,):\n",
    "    f_cell='A30'\n",
    "    f_cell2='A31'\n",
    "    save_table(sheet,table)\n",
    "    if sheet.range(f_cell).value is None:\n",
    "        sheet.range(f_cell).formula =f\"={f1}\"          \n",
    "        sheet.range(f_cell2).formula =f\"={f2}\"\n",
    "        \n",
    "        result1 = sheet.range(f_cell).value\n",
    "        result2 = sheet.range(f_cell2).value\n",
    "        print(f1,\"\\n\",f2,\"\\n\\n\")\n",
    "        \n",
    "        if result1 is None:\n",
    "            return False,result1,result2\n",
    "        if result1 is not None and result2 is not None:\n",
    "            if result1==result2:\n",
    "                item['exe_results']=result1\n",
    "\n",
    "                return True,result1,result2\n",
    "\n",
    "        else:\n",
    "            print('Fail to get the answer')   \n",
    "    else:\n",
    "        raise ValueError(\"Cell not Empty!\")\n",
    "    return False,result1,result2\n",
    "\n",
    "def run_execution_test(test_list,sheet=None):\n",
    "    count,count_true=0,0\n",
    "    em,exe=0,0\n",
    "    count_wronggd=0\n",
    "    fail_list=[]\n",
    "    #count_true_em,count_true_exe=0,0\n",
    "    for item in test_list[:]:\n",
    "        item['res']=\"failed\"\n",
    "        pre_f = item.get('pre_f')\n",
    "        if pre_f is None:\n",
    "            item['res']='not found'\n",
    "            continue\n",
    "        count+=1\n",
    "        formula2 = item['Formula2']\n",
    "        a=formula2.lower().replace(\" \", \"\").replace(\"\\\"\", \"\")\n",
    "        b=pre_f.lower().replace(\" \", \"\").replace(\"\\\"\", \"\")\n",
    "        nl=item['Question']\n",
    "        if a==b:   \n",
    "            item['res']='exact_match'\n",
    "            count_true+=1\n",
    "            em+=1\n",
    "            #print(formula2,\"\\n\",pre_f)\n",
    "            print(count,'exact_match')\n",
    "        else:\n",
    "            flag,gdres,llmres=eval_execution1(sheet,item,item['Table'],formula2,pre_f)\n",
    "            if(flag):\n",
    "                exe+=1\n",
    "                item['res']='exe_match'\n",
    "                count_true+=1\n",
    "    print(\"em\",em)\n",
    "    print(\"exe\",exe)\n",
    "    return fail_list,count_true/count,count_wronggd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import json\n",
    "\n",
    "app = xw.App(visible=True)\n",
    "\n",
    "\n",
    "book = app.books.open(r\"b1.xlsx\")\n",
    "sheet = book.sheets[0] \n",
    "\n",
    "fail_list,percentage,wg=run_execution_test(data,sheet)\n",
    "print(percentage)\n",
    "\n",
    "book.close()\n",
    "app.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_list=test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "counters = {\n",
    "    'easy': {'exact_match': 0, 'exe_match': 0,'failed': 0},\n",
    "    'medium': {'exact_match': 0, 'exe_match': 0,'failed': 0},\n",
    "    'hard': {'exact_match': 0, 'exe_match': 0,'failed': 0},\n",
    "    'Calculation': {'exact_match': 0, 'exe_match': 0,'failed': 0},\n",
    "}\n",
    "\n",
    "\n",
    "for item in res_list:\n",
    "    if item is None:\n",
    "        continue\n",
    "    level = item['Level']\n",
    "    result = item['res']\n",
    "\n",
    "    counters[level][result] += 1\n",
    "\n",
    "for level, results in counters.items():\n",
    "    print(f\"Level: {level}\")\n",
    "    total = sum(results.values())\n",
    "    for result, count in results.items():\n",
    "        percentage = (count / total) * 100\n",
    "        print(f\"{result}: {count} ({percentage:.2f}%)\")\n",
    "        \n",
    "df_res_l=pd.DataFrame(counters).T\n",
    "df_res_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import importlib,utils,json,os\n",
    "from utils import *\n",
    "gpt_eval=utils.read_json(\"gpt_eval.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "counters = {\n",
    "    'easy': {'exact_match': 0, 'exe_match': 0,'failed': 0},\n",
    "    'medium': {'exact_match': 0, 'exe_match': 0,'failed': 0},\n",
    "    'hard': {'exact_match': 0, 'exe_match': 0,'failed': 0},\n",
    "    'Calculation': {'exact_match': 0, 'exe_match': 0,'failed': 0},\n",
    "}\n",
    "\n",
    "\n",
    "for item in res_list:\n",
    "    level = item['Level']\n",
    "    result = item['res']\n",
    "    # if(result!=\"failed\"):\n",
    "    #     print(result)\n",
    "    counters[level][result] += 1\n",
    "\n",
    "\n",
    "for level, results in counters.items():\n",
    "    print(f\"Level: {level}\")\n",
    "    total = sum(results.values())\n",
    "    for result, count in results.items():\n",
    "        percentage = (count / total) * 100\n",
    "        print(f\"{result}: {count} ({percentage:.2f}%)\")\n",
    "        \n",
    "df_res_l=pd.DataFrame(counters).T\n",
    "df_res_l\n",
    "\n",
    "df_res_l[\"em_rate\"]=df_res['exact_match']/(df_res['exact_match']+df_res[\"failed\"])\n",
    "print(df_res_l['exact_match'].sum()/(df_res_l['exact_match'].sum()+df_res_l['failed'].sum()))\n",
    "df_res_l\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "formular",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
